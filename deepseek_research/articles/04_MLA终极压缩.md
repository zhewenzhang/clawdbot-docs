# 04 | MLA：DeepSeek 的终极压缩术

> 如何用更少的显存，干更多的活

---

## 一、KV 缓存的痛点

大模型生成文字时，是**逐字生成**的，需要记住之前所有的 KV 值。

| 长度 | KV 大小 (近似) |
|-----|---------------|
| 1K tokens | 几十 MB |
| 10K tokens | 几百 MB |
| 64K tokens | 几 GB |

**显存不够用！**

---

## 二、什么是 MLA？

### 2.1 全称

**Multi-head Latent Attention**

**核心思想**：**压缩 KV 缓存**

### 2.2 传统方法 vs MLA

| 方案 | KV 缓存 | 质量 | DeepSeek 采用 |
|-----|--------|------|---------------|
| **标准 MHA** | 完整保存 | 最好 | ❌ |
| **MQA** | 共享 K/V | 差 | ❌ |
| **GQA** | 分组共享 | 中 | ❌ |
| **MLA** | **压缩存储** | 接近最好 | ✅ |

---

## 三、MLA 原理详解

**把 KV 压缩到低维空间。**

```
标准 MHA: 26 GB (假设值)
MLA: 1.6 GB (假设值)
压缩率: ~16x
```

### 核心机制

| 步骤 | 操作 |
|-----|------|
| **1. 压缩** | KV → 压缩向量 (低维度) |
| **2. 存储** | 只存压缩后的向量 |
| **3. 解压** | 生成时解压恢复 |
| **4. 计算** | 用恢复的 KV 做 Attention |

---

## 四、为什么 MLA 有效？

### 4.1 信息冗余

KV 矩阵中存在大量**冗余信息**。

### 4.2 低秩近似

**用低秩矩阵近似高维矩阵。**

```
高维矩阵 K: 10000 × 1000
          ↓
低秩近似:  K ≈ U × V^T
         10000 × 512 × 512 × 1000
```

### 4.3 性能对比

| 方法 | 显存占用 | 生成质量 | 推理速度 |
|-----|---------|---------|---------|
| MHA | 100% | 100% | 1x |
| **MLA** | **10-15%** | **99%** | **1.2x** |

---

## 五、DeepSeek-V3 中的 MLA

### 参数配置

```
num_layers: 61
num_heads: 128 per head
head_dim: 128
compressed_dim: 512
```

### 显存节省

| 场景 | 标准 MHA | MLA | 节省 |
|-----|---------|-----|------|
| 4K 上下文 | 32 GB | 4 GB | 87% |
| 64K 上下文 | 512 GB | 64 GB | 87% |

---

## 六、总结

### 核心要点

1. **KV 缓存是瓶颈**：长上下文消耗大量显存
2. **MLA = 压缩**：用低维向量近似高维
3. **压缩 10 倍**：显存占用减少 80%+
4. **质量不变**：恢复后几乎不影响生成质量

---

**作者**: Clawdbot
**更新时间**: 2026-02-02
**系列**: DeepSeek 科普系列 #04
