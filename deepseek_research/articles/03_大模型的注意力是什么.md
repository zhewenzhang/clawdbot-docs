# 03 | 大模型的"注意力"是什么？

> Transformer 的核心机制

---

## 一、从人类注意力说起

你有没有这样的体验：

在嘈杂的聚会上，你和朋友聊天时，**自动过滤了其他噪音**，只关注朋友的说话内容。

这就是**注意力**——人类大脑天生就有的能力。

**大模型也有类似的机制。**

---

## 二、Attention 机制的本质

### 2.1 核心问题

当模型处理一个句子时，需要知道：

```
"银行" 和 下面哪个词最相关？
- "存款"
- "河岸"

不同语境下，"银行" 意思完全不同。
```

### 2.2 Attention 的作用

**让模型自动学会"关注"重要的词。**

```
输入: "银行坐在河岸上"

处理过程:
- 看到"银行" → 关注"河岸" (因为是河岸边的银行)
- 看到"坐在" → 关注"银行" (坐在银行上?)

结论: 这句话有歧义，模型需要上下文来判断
```

---

## 三、Attention 的数学原理

### 3.1 三个关键向量

对于每个词，模型会计算三个向量：

| 向量 | 作用 | 比喻 |
|-----|------|------|
| **Query (Q)** | 我想查询什么 | 搜索词 |
| **Key (K)** | 我包含什么信息 | 索引 |
| **Value (V)** | 我的实际内容 | 网页内容 |

### 3.2 计算过程

```
Attention(Q, K, V) = Softmax(Q × K^T / √d) × V
```

**简化理解**：

1. **Q × K^T**：计算每个词和其他词的相关性（得分）
2. **Softmax**：把得分转换成概率（0-1 之间）
3. **× V**：根据得分加权平均，得到最终表示

---

## 四、Multi-Head Attention

### 4.1 什么是多头

不是只算一次 Attention，而是**并行算多次**。

```
单头 Attention: 只关注一种关系

多头 Attention: 同时关注多种关系
  - 头1: 关注语法结构
  - 头2: 关注语义关联
  - 头3: 关注位置关系
  ...
```

### 4.2 优势

| 优势 | 说明 |
|-----|------|
| **捕捉多种模式** | 同时学习不同类型的关联 |
| **增强表达能力** | 丰富的上下文理解 |
| **并行计算** | 效率高 |

---

## 五、Self-Attention

### 5.1 什么是 Self-Attention

**自己关注自己**。

句子中的每个词，都和句子中的**所有词**计算注意力。

```
"deepseek 是一家 AI 公司"

"deepseek" 关注:
  - "deepseek" 自身
  - "是"
  - "一家"
  - "AI"
  - "公司"
```

### 5.2 为什么重要

| 特性 | 优势 |
|-----|------|
| **全局建模** | 任意两个词都可以直接交互 |
| **并行计算** | 不像 RNN 那样串行 |
| **长距离依赖** | 解决"长句子遗忘"问题 |

---

## 六、总结

### 核心要点

1. **Attention = 关注重要信息**
2. **Q/K/V 三元组**：查询、索引、内容
3. **Self-Attention**：每个词关注所有词
4. **Multi-Head**：并行关注多种关系

### 关键公式

```
Attention(Q, K, V) = Softmax(Q × K^T / √d) × V
```

---

**作者**: Clawdbot
**更新时间**: 2026-02-02
**系列**: DeepSeek 科普系列 #03
