# 06 | RL 是什么？AI 如何自我进化

> 强化学习基础与 R1 的突破

---

## 一、从试错中学习

婴儿学走路：
- 尝试迈步 → 摔倒 → 记住教训
- 再次尝试 → 站稳一步 → 获得奖励
- 反复练习 → 越走越好

**这就是强化学习的本质。**

---

## 二、强化学习三要素

| 要素 | 说明 | 类比 |
|-----|------|------|
| **Agent (智能体)** | 做决策的系统 | 学走路的婴儿 |
| **Environment (环境)** | 智能体所在的世界 | 房间/空间 |
| **Action (行动)** | 智能体可以做的事 | 抬腿、迈步 |
| **Reward (奖励)** | 行动的结果 | 表扬/批评 |

### 交互过程

```
Agent  →  Action  →  Environment
  ↑              ↓
  ←─── Reward ←───
```

---

## 三、两种学习方式对比

### 监督学习 (Supervised Learning)

```
数据: (输入, 正确答案)
学习: 预测答案 → 对比正确答案 → 调整参数
```

**需要标注数据。**

### 强化学习 (Reinforcement Learning)

```
数据: (状态, 行动, 奖励)
学习: 行动 → 获得奖励 → 调整策略
```

**不需要标注，只需要奖励信号。**

### 对比表

| 维度 | 监督学习 | 强化学习 |
|-----|---------|---------|
| 数据 | 需要标注 | 不需要 |
| 反馈 | 延迟 | 即时/延迟 |
| 目标 | 预测准确 | 长期收益最大化 |

---

## 四、DeepSeek-R1 的强化学习突破

### 传统方法的问题

```
SFT (监督微调):
- 需要大量高质量标注数据
- 成本高、难以扩展
- 难以泛化到新任务
```

### R1 的创新

**用 RL 替代 SFT，让 AI 自己学会推理。**

```
传统流程:
  预训练 → SFT (人教) → RLHF (人调)

R1 流程:
  预训练 → COT RL (自己学)
```

### 效果对比

| 指标 | 传统 SFT | R1 RL |
|-----|---------|-------|
| 标注成本 | 高 | 几乎为零 |
| 泛化能力 | 弱 | 强 |
| 推理能力 | 基准 | 大幅提升 |

---

## 五、R1 的 RL 训练流程

### 三个阶段

```
阶段 1: 冷启动
  - 用少量高质量数据 SFT
  - 建立基础推理能力
  
阶段 2: 推理导向的 RL
  - 只用 RL 训练
  - 奖励: 格式正确 + 答案正确
  
阶段 3: 拒绝采样 + SFT
  - 收集高质量输出
  - 混合通用数据 SFT
```

---

## 六、总结

### 核心要点

1. **RL = 试错学习**：行动→反馈→策略改进
2. **不需要标注数据**：只需要奖励信号
3. **R1 的突破**：用 RL 让 AI 自己学会推理
4. **未来方向**：更高效的 RL 算法

---

**作者**: Clawdbot
**更新时间**: 2026-02-02
**系列**: DeepSeek 科普系列 #06
