# 07 | GRPO：让 AI 自己学会推理

> 组相对策略优化详解

---

## 一、PPO 的问题

**Proximal Policy Optimization**

**近端策略优化**

当前最流行的 RL 算法之一。

### PPO 的局限

| 问题 | 说明 |
|-----|------|
| **需要价值模型** | 计算复杂 |
| **超参数敏感** | KL 散度系数难调 |
| **样本效率低** | 需要大量采样 |

---

## 二、GRPO 的创新

### 2.1 全称

**Group Relative Policy Optimization**

**组相对策略优化**

### 2.2 核心思想

**不要绝对评价，要相对比较。**

```
PPO (绝对评价):
这个答案: 85 分 (需要价值模型)

GRPO (相对比较):
这组答案中:
  A: 第1名
  B: 第3名
  C: 第2名
  D: 第4名
  
不需要知道具体分数，只需要知道排名！
```

---

## 三、GRPO 原理详解

### 3.1 组采样

**一次生成多个答案。**

```
输入问题: "1+1=?"
     ↓
生成 N 个答案: [2, 2, 2, 3, 2, 2, 2, 2]
     ↓
计算每个答案的奖励
```

### 3.2 奖励计算

| 步骤 | 操作 |
|-----|------|
| **1. 正确答案** | 2 是正确答案 |
| **2. 奖励分配** | 答对得 1 分，答错得 0 分 |
| **3. 归一化** | 用组内统计量归一化 |
| **4. 计算优势** | 相对组平均的优势 |

### 3.3 公式

```
优势 = (奖励 - 组均值) / 组标准差
```

**好处**：不需要价值模型，用组内统计量代替！

---

## 四、GRPO vs PPO

### 核心差异

| 维度 | PPO | GRPO |
|-----|-----|------|
| 评价方式 | 绝对分数 | 相对排名 |
| 价值模型 | 需要 | 不需要 |
| 样本效率 | 低 | 高 |
| 实现复杂度 | 高 | 低 |

---

## 五、为什么 GRPO 适合 LLM 训练

### LLM 的特点

| 特点 | 说明 |
|-----|------|
| **生成成本高** | 每次只生成一个 token |
| **可批量生成** | 一次生成多个答案 |
| **有明确答案** | 数学、代码有客观标准 |

### GRPO 的优势

| 优势 | 说明 |
|-----|------|
| **批量利用** | 一次生成多个，GPU 利用率高 |
| **无需价值模型** | LLM 本身作为评判 |
| **对比学习** | 相对评价更稳定 |

---

## 六、DeepSeek-R1 中的 GRPO

### 奖励设计

| 奖励类型 | 条件 | 分数 |
|---------|------|------|
| **准确率奖励** | 答案正确 | +1 |
| **格式奖励** | 符合 COT 格式 | +0.1 |
| **惩罚** | 格式错误 | -0.1 |

### 训练效果

```
训练前: 数学题准确率 30%
训练后: 数学题准确率 92% (+62%)
```

---

## 七、总结

### 核心要点

1. **GRPO = 组相对策略优化**
2. **核心思想**：相对比较代替绝对评价
3. **不需要价值模型**：用组统计量代替
4. **适合 LLM**：批量生成、对比学习

---

**作者**: Clawdbot
**更新时间**: 2026-02-02
**系列**: DeepSeek 科普系列 #07
