# NVIDIA GPU Roadmap (2024-2026)

**æ›´æ–°æ—¶é—´**: 2026-02-03  
**æ•°æ®æ¥æº**: NVIDIAå®˜æ–¹ã€TechTechã€Wikipediaã€è¡Œä¸šåˆ†ææŠ¥å‘Š

---

## ğŸ“Š æ¶æ„æ¼”è¿›æ—¶é—´çº¿

| æ¶æ„ | å‘å¸ƒæ—¶é—´ | åˆ¶ç¨‹èŠ‚ç‚¹ | ä»£è¡¨äº§å“ | å…³é”®ç‰¹æ€§ |
|------|----------|----------|----------|----------|
| **Ampere** | 2020 Q2 | 7nm (Samsung 8nm) | A100, RTX 3090 | ç¬¬ä¸‰ä»£Tensor Core, PCIe 4.0 |
| **Ada Lovelace** | 2022 Q4 | 4nm (TSMC N4) | RTX 4090, L40S | ç¬¬å››ä»£Tensor Core, DLSS 3 |
| **Hopper** | 2022 Q4 | 4nm (TSMC N4) | H100, H200 | ç¬¬å››ä»£Tensor Core, Transformer Engine |
| **Blackwell** | 2024 Q2 | 4nm (TSMC N4P) | B100, B200, GB200 | ç¬¬äº”ä»£Tensor Core, 8å †æ ˆHBM3e |
| **Blackwell Ultra** | 2025 H2 | 4nm (TSMC N4P) | B200A, GB300 | å¢å¼ºç‰ˆBlackwell |
| **Rubin** | 2026 H2 | 3nm (TSMC N3P) | R100, R200 | ç¬¬å…­ä»£Tensor Core, 288GB HBM4 |
| **Rubin Ultra** | 2027 H2 | 3nm (TSMC N3P) | RU Ultra | 12-Hi HBM4å †æ ˆ |

---

## ğŸ¯ ä¸»è¦äº§å“è§„æ ¼å¯¹æ¯”

### æ•°æ®ä¸­å¿ƒGPUç³»åˆ—

| äº§å“ | å‘å¸ƒæ—¶é—´ | åˆ¶ç¨‹ | HBMè§„æ ¼ | CUDAæ ¸å¿ƒæ•° | TDP | FP8æ€§èƒ½ |
|------|----------|------|---------|------------|-----|---------|
| **H100** | 2023 Q1 | TSMC N4 | 80GB HBM3 | 16896 | 700W | 2000 TFLOPS |
| **H200** | 2024 Q1 | TSMC N4 | 141GB HBM3e | 16896 | 800W | 3000 TFLOPS |
| **B100** | 2024 Q2 | TSMC N4P | 192GB HBM3e | 18000 | 700W | 4500 TFLOPS |
| **B200** | 2024 Q2 | TSMC N4P | 192GB HBM3e | 18000 | 1000W | 9000 TFLOPS |
| **GB200 NVL72** | 2024 Q3 | TSMC N4P | 8å †æ ˆHBM3e | - | - | 1.8 EFLOPS |
| **Rubin R100** | 2026 H2 | TSMC N3P | 288GB HBM4 | TBD | TBD | 50 PFLOPS(FP4) |

### CoWoSå°è£…éœ€æ±‚å˜åŒ–

| äº§å“ | CoWoSå±‚æ•° | Interposerå°ºå¯¸ | äº§èƒ½éœ€æ±‚ |
|------|-----------|----------------|----------|
| H100 | 2.5D CoWoS-S | ~2x reticle | ä¸­ç­‰ |
| B100/B200 | CoWoS-L | ~3x reticle | é«˜ |
| GB200 | CoWoS-L + 3D V-Cache | 4x reticle | æé«˜ |
| Rubin | CoWoS-L (æ–°ä¸€ä»£) | 9.5x reticle (2027) | æé«˜ |

---

## ğŸ”— å…³é”®è§„æ ¼æ¼”è¿›

### HBMå®¹é‡æ¼”è¿›
- **2020 (Ampere)**: 40-80GB HBM2e
- **2022 (Hopper)**: 80GB HBM3
- **2024 (Blackwell)**: 192GB HBM3e
- **2026 (Rubin)**: 288GB HBM4

### äº’è¿å¸¦å®½æ¼”è¿›
- **NVLink 4**: 900 GB/s (Blackwell)
- **NVLink 5**: 1.8 TB/s (Blackwell Ultra/ Rubin)
- **ConnectX-8**: 800 Gb/s

---

## ğŸ“ˆ å¸‚åœºå®šä½ä¸ç«äº‰åˆ†æ

### äº§å“å®šä½çŸ©é˜µ
| å±‚çº§ | äº§å“ | ç›®æ ‡å¸‚åœº | ç«äº‰å¯¹è±¡ |
|------|------|----------|----------|
| æ——èˆ°è®­ç»ƒ | GB200 NVL72 | LLMè®­ç»ƒã€è¶…å¤§è§„æ¨¡AI | æ— ç›´æ¥ç«äº‰ |
| ä¸»åŠ›è®­ç»ƒ | B200/H200 | ä¼ä¸šAIè®­ç»ƒ | AMD MI350X |
| æ¨ç†ä¼˜åŒ– | B100 | AIæ¨ç†ã€è¾¹ç¼˜è®¡ç®— | AMD MI300X |
| å…¥é—¨çº§ | L40S | è¾¹ç¼˜æ¨ç†ã€æ¸²æŸ“ | NVIDIA RTX 6000 |

### å¸‚åœºä»½é¢
- **æ•°æ®ä¸­å¿ƒGPU**: ~80-90%å¸‚åœºä»½é¢ (2024)
- **AIåŠ é€Ÿå™¨**: ä¸»å¯¼åœ°ä½ï¼Œé¢ä¸´AMDå’Œè‹±ç‰¹å°”ç«äº‰

---

## ğŸ’¡ å…³é”®æ´å¯Ÿ

1. **æ¶æ„è¿­ä»£åŠ é€Ÿ**: ä»2å¹´ä¸€ä»£åŠ é€Ÿåˆ°1-1.5å¹´ä¸€ä»£ï¼Œä»¥åº”å¯¹AIéœ€æ±‚çˆ†å‘
2. **HBM4æˆä¸º2026å¹´å…³é”®**: Rubinå°†é¦–å‘288GB HBM4ï¼Œå¸¦å®½å’Œå®¹é‡å¤§å¹…æå‡
3. **CoWoSäº§èƒ½ç“¶é¢ˆ**: 2024-2025å¹´CoWoSäº§èƒ½æŒç»­ç´§å¼ ï¼ŒNVIDIAç§¯ææ¨åŠ¨äº§èƒ½æ‰©å¼ 
4. **3nmåˆ¶ç¨‹åˆ‡æ¢**: 2026å¹´Rubinå°†åˆ‡æ¢è‡³TSMC 3nmåˆ¶ç¨‹ï¼Œæ€§èƒ½æå‡çº¦15-20%
5. **Rubin Ultraå‰ç»**: 2027å¹´å°†æ¨å‡º12-Hi HBM4å †æ ˆç‰ˆæœ¬ï¼Œæ€§èƒ½è¿›ä¸€æ­¥ç¿»å€

---

## ğŸ”— å‚è€ƒæ¥æº

1. [NVIDIA Blackwell Architecture](https://en.wikipedia.org/wiki/Blackwell_(microarchitecture)) - Wikipedia
2. [NVIDIA GPU Upgrade Planning](https://www.cudocompute.com/blog/nvidia-gpu-upgrade-planning) - Cudo Compute
3. [NVIDIA GPU System Roadmap to 2028](https://www.nextplatform.com/2025/03/19/nvidia-draws-gpu-system-roadmap-out-to-2028/) - The Next Platform
4. [NVIDIA Unfolds GPU Roadmap to 2027](https://www.nextplatform.com/2024/06/02/nvidia-unfolds-gpu-interconnect-roadmaps-out-to-2027/) - The Next Platform
5. [TSMC CoWoS Capacity Expansion](https://www.trendforce.com/news/2025/01/02/news-tsmc-set-to-expand-cowos-capacity-to-record-75000-wafers-in-2025-doubling-2024-output/) - TrendForce

---

*æ–‡æ¡£ç»´æŠ¤: åŠå¯¼ä½“Roadmapæ·±åº¦æ•´ç†é¡¹ç›®*  
*æœ€åæ›´æ–°: 2026-02-03*
