# å¤§å‹PDFå¤„ç†ä¼˜åŒ–æ–¹æ¡ˆ

**åˆ›å»ºæ—¥æœŸ**: 2026-02-08
**èƒŒæ™¯**: å¤„ç†æ¬£å…´ç”µå­2024å¹´å¹´æŠ¥ï¼ˆ4.1MB, 122é¡µï¼‰
**ç›®æ ‡**: å»ºç«‹é«˜æ•ˆçš„å¤§å‹PDFå¤„ç†å·¥ä½œæµ

---

## ä¸€ã€PDFä¸‹è½½ä¼˜åŒ–æ–¹æ¡ˆ

### 1.1 åˆ†æ®µä¸‹è½½ï¼ˆé’ˆå¯¹>10MBæ–‡ä»¶ï¼‰

```python
# download_pdf_stream.py
import requests
import os
from tqdm import tqdm

classåˆ†æ®µä¸‹è½½å™¨:
    """æ”¯æŒæ–­ç‚¹ç»­ä¼ å’Œåˆ†æ®µä¸‹è½½çš„PDFä¸‹è½½å™¨"""
    
    def __init__(self, url, part_size_mb=5, output_dir="/tmp"):
        self.url = url
        self.part_size = part_size_mb * 1024 * 1024  # è½¬æ¢ä¸ºå­—èŠ‚
        self.output_dir = output_dir
        self.temp_files = []
        
    defè·å–æ–‡ä»¶å¤§å°(self):
        """è·å–è¿œç¨‹æ–‡ä»¶æ€»å¤§å°"""
        r = requests.head(self.url)
        return int(r.headers.get('content-length', 0))
    
    defä¸‹è½½åˆ†æ®µ(self, start, end, part_num):
        """ä¸‹è½½æ–‡ä»¶çš„æŒ‡å®šåˆ†æ®µ"""
        headers = {"Range": f"bytes={start}-{end}"}
        r = requests.get(self.url, headers=headers, stream=True)
        
        part_path = os.path.join(self.output_dir, f"part_{part_num}.pdf")
        self.temp_files.append(part_path)
        
        with open(part_path, "wb") as f:
            for chunk in tqdm(r.iter_content(chunk_size=8192), 
                            desc=f"Part {part_num}"):
                f.write(chunk)
        
        return part_path
    
    defåˆå¹¶æ–‡ä»¶(self, output_path):
        """åˆå¹¶æ‰€æœ‰åˆ†æ®µ"""
        with open(output_path, "wb") as out:
            for i, part in enumerate(sorted(self.temp_files), 1):
                with open(part, "rb") as f:
                    out.write(f.read())
                os.remove(part)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        
        self.temp_files.clear()
        return output_path
    
    defä¸‹è½½(self, output_path):
        """æ‰§è¡Œåˆ†æ®µä¸‹è½½"""
        total_size = self.è·å–æ–‡ä»¶å¤§å°()
        
        if total_size < self.part_size:
            # å°æ–‡ä»¶ç›´æ¥ä¸‹è½½
            r = requests.get(self.url, stream=True)
            with open(output_path, "wb") as f:
                for chunk in tqdm(r.iter_content(chunk_size=8192)):
                    f.write(chunk)
        else:
            # åˆ†æ®µä¸‹è½½
            num_parts = (total_size // self.part_size) + 1
            for i in range(num_parts):
                start = i * self.part_size
                end = min((i + 1) * self.part_size - 1, total_size - 1)
                self.ä¸‹è½½åˆ†æ®µ(start, end, i + 1)
            
            self.åˆå¹¶æ–‡ä»¶(output_path)
        
        return output_path

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    downloader = åˆ†æ®µä¸‹è½½å™¨(
        url="https://example.com/large_report.pdf",
        part_size_mb=5
    )
    downloader.ä¸‹è½½("/tmp/large_report.pdf")
```

### 1.2 æ¸è¿›å¼ä¸‹è½½ï¼ˆä»…ä¸‹è½½éœ€è¦çš„é¡µé¢ï¼‰

```python
# download_pages.py
import requests

defä¸‹è½½æŒ‡å®šé¡µé¢(url, page_numbers, output_dir="/tmp"):
    """
    ä»…ä¸‹è½½PDFçš„æŒ‡å®šé¡µé¢ï¼ˆæ— éœ€ä¸‹è½½æ•´ä¸ªæ–‡ä»¶ï¼‰
    ä½¿ç”¨PDFiumæœåŠ¡å™¨ç«¯æ¸²æŸ“
    """
    results = []
    
    for page_num in page_numbers:
        # è½¬æ¢ä¸º0-basedç´¢å¼•
        page_index = page_num - 1
        
        # æ–¹æ³•1: ä½¿ç”¨Google Docs Viewer
        viewer_url = f"https://r.jina.ai/http://{url}"
        # è¿™ä¸ªæœåŠ¡ä¼šæå–æ–‡æœ¬ä½†ä¸æ”¯æŒé¡µé¢é€‰æ‹©
        
        # æ–¹æ³•2: ä½¿ç”¨åœ¨çº¿PDF API
        api_url = f"https://pdf-api.herokuapp.com/pdf/{url}/page/{page_num}"
        
        # æ–¹æ³•3: æœ¬åœ°å¤„ç†ï¼ˆæ¨èï¼‰
        # å…ˆä¸‹è½½æ•´ä¸ªæ–‡ä»¶ï¼Œç„¶åç”¨PyMuPDFæå–æŒ‡å®šé¡µé¢
        
        results.append(f"Page {page_num}: å¾…å®ç°")
    
    return results
```

---

## äºŒã€PDFå¤„ç†ä¼˜åŒ–æ–¹æ¡ˆ

### 2.1 æ¨èå·¥å…·å¯¹æ¯”

| å·¥å…·/åº“ | é€Ÿåº¦ | å†…å­˜æ•ˆç‡ | é€‚ç”¨åœºæ™¯ | å®‰è£…æ–¹å¼ |
|---------|------|---------|---------|---------|
| **PyMuPDF (fitz)** | â­â­â­â­â­ | â­â­â­â­â­ | é«˜æ€§èƒ½æå–ã€æˆªå›¾ | `pip install pymupdf` |
| **pdfplumber** | â­â­â­ | â­â­â­ | è¡¨æ ¼æå– | `pip install pdfplumber` |
| **pdfminer.six** | â­â­ | â­â­ | è¯¦ç»†æ–‡æœ¬åˆ†æ | `pip install pdfminer.six` |
| **pypdf** | â­â­â­ | â­â­â­ | åˆå¹¶/æ‹†åˆ† | `pip install pypdf` |
| **Tabula-py** | â­â­â­ | â­â­ | è¡¨æ ¼è¯†åˆ« | `pip install tabula-py` |
| **pdfimages** | â­â­â­â­â­ | â­â­â­â­ | å›¾åƒæå– | `brew install poppler` |

### 2.2 PyMuPDFé«˜æ€§èƒ½æå–æ¨¡æ¿

```python
# pdf_processor.py
import fitz  # PyMuPDF
import os
from pathlib import Path
import json

classé«˜æ•ˆPDFå¤„ç†å™¨:
    """åŸºäºPyMuPDFçš„é«˜æ€§èƒ½PDFå¤„ç†å™¨"""
    
    def __init__(self, pdf_path):
        self.doc = fitz.open(pdf_path)
        self.page_count = len(self.doc)
        self.metadata = self.doc.metadata
        
    defæå–æ–‡æœ¬(self, page_numbers=None):
        """æå–æŒ‡å®šé¡µé¢æˆ–å…¨éƒ¨æ–‡æœ¬"""
        texts = {}
        
        pages = page_numbers if page_numbers else range(1, self.page_count + 1)
        
        for page_num in pages:
            page = self.doc[page_num - 1]  # 0-basedç´¢å¼•
            text = page.get_text()
            texts[page_num] = text
        
        return texts
    
    defæå–è¡¨æ ¼(self, page_numbers=None):
        """æå–è¡¨æ ¼ï¼ˆä½¿ç”¨è¡¨æ ¼æ£€æµ‹ï¼‰"""
        tables = {}
        
        pages = page_numbers if page_numbers else range(1, self.page_count + 1)
        
        for page_num in pages:
            page = self.doc[page_num - 1]
            
            # è·å–è¡¨æ ¼åŒºåŸŸ
            tables_on_page = page.find_tables()
            
            if tables_on_page:
                tables[page_num] = []
                for table in tables_on_page:
                    table_data = table.extract()
                    tables[page_num].append(table_data)
        
        return tables
    
    defæˆªå›¾æŒ‡å®šé¡µé¢(self, page_numbers, output_dir="/tmp", prefix="page"):
        """æˆªå–æŒ‡å®šé¡µé¢ä¸ºå›¾ç‰‡"""
        Path(output_dir).mkdir(parents=True, exist_ok=True)
        
        saved_files = []
        
        for page_num in page_numbers:
            if 1 <= page_num <= self.page_count:
                page = self.doc[page_num - 1]
                
                # æ¸²æŸ“ä¸ºå›¾ç‰‡ï¼ˆ300 DPIä»¥è·å¾—æ¸…æ™°åº¦ï¼‰
                pix = page.get_pixmap(matrix=fitz.Matrix(300/72, 300/72))
                
                output_path = os.path.join(output_dir, f"{prefix}_{page_num}.png")
                pix.save(output_path)
                saved_files.append(output_path)
                
                print(f"âœ… ä¿å­˜: {output_path}")
        
        return saved_files
    
    defæœç´¢å…³é”®è¯(self, keywords):
        """æœç´¢å…³é”®è¯ï¼Œè¿”å›åŒ¹é…çš„é¡µé¢å’Œä½ç½®"""
        results = {}
        
        for page_num in range(1, self.page_count + 1):
            page = self.doc[page_num - 1]
            text = page.get_text()
            
            matches = {}
            for keyword in keywords:
                if keyword.lower() in text.lower():
                    # æ‰¾åˆ°æ‰€æœ‰å‡ºç°ä½ç½®
                    positions = []
                    start = 0
                    while True:
                        pos = text.lower().find(keyword.lower(), start)
                        if pos == -1:
                            break
                        positions.append(pos)
                        start = pos + 1
                    
                    matches[keyword] = positions
            
            if matches:
                results[page_num] = matches
        
        return results
    
    defå¿«é€Ÿæå–å¤§é™†å­å…¬å¸ä¿¡æ¯(self):
        """å¿«é€Ÿæå–è´¢æŠ¥ä¸­çš„å¤§é™†å­å…¬å¸ä¿¡æ¯"""
        # æœç´¢å…³é”®è¯
        keywords = [
            "å¤§é™¸å­å…¬å¸",
            "å¤§é™¸æŠ•è³‡",
            "åœ°å€è¢«æŠ•è³‡å…¬å¸",
            "é™„å±¬å…¬å¸"
        ]
        
        search_results = self.æœç´¢å…³é”®è¯(keywords)
        
        # æå–åŒ¹é…é¡µé¢
        target_pages = list(search_results.keys())
        
        # æå–è¿™äº›é¡µé¢çš„è¡¨æ ¼
        tables = self.æå–è¡¨æ ¼(target_pages)
        
        # æå–è¿™äº›é¡µé¢çš„æ–‡æœ¬
        texts = self.æå–æ–‡æœ¬(target_pages)
        
        return {
            "search_results": search_results,
            "tables": tables,
            "texts": texts,
            "target_pages": target_pages
        }
    
    defå…³é—­(self):
        """å…³é—­PDFæ–‡ä»¶"""
        self.doc.close()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    processor = é«˜æ•ˆPDFå¤„ç†å™¨("/tmp/unimicron_2024_tw.pdf")
    
    # å¿«é€Ÿæå–å¤§é™†å­å…¬å¸ä¿¡æ¯
    results = processor.å¿«é€Ÿæå–å¤§é™†å­å…¬å¸ä¿¡æ¯()
    print(f"æ‰¾åˆ° {len(results['target_pages'])} ä¸ªç›¸å…³é¡µé¢")
    
    # æˆªå›¾ä¿å­˜
    if results['target_pages']:
        processor.æˆªå›¾æŒ‡å®šé¡µé¢(results['target_pages'], prefix="å¤§é™†å­å…¬å¸")
    
    processor.å…³é—­()
```

### 2.3 è¡¨æ ¼æå–å¢å¼ºæ–¹æ¡ˆ

```python
# table_extractor.py
import fitz
import pandas as pd

classè´¢æŠ¥è¡¨æ ¼æå–å™¨:
    """ä¸“é—¨ç”¨äºæå–è´¢æŠ¥è¡¨æ ¼"""
    
    def __init__(self, pdf_path):
        self.doc = fitz.open(pdf_path)
        
    defæå–è´¢åŠ¡æŠ¥è¡¨(self):
        """æå–èµ„äº§è´Ÿå€ºè¡¨ã€æŸç›Šè¡¨ç­‰è´¢åŠ¡æŠ¥è¡¨"""
        reports = {}
        
        # æœç´¢è´¢åŠ¡æŠ¥è¡¨å…³é”®è¯
        report_keywords = {
            "åˆä½µè³‡ç”¢è² å‚µè¡¨": "èµ„äº§è´Ÿå€ºè¡¨",
            "åˆä½µç¶œåˆæç›Šè¡¨": "ç»¼åˆæŸç›Šè¡¨", 
            "åˆä½µç¾é‡‘æµé‡è¡¨": "ç°é‡‘æµé‡è¡¨",
            "æ¬Šç›Šè®Šå‹•è¡¨": "æƒç›Šå˜åŠ¨è¡¨"
        }
        
        for page_num in range(1, len(self.doc) + 1):
            page = self.doc[page_num - 1]
            text = page.get_text()
            
            for keyword, report_type in report_keywords.items():
                if keyword in text:
                    if report_type not in reports:
                        reports[report_type] = []
                    reports[report_type].append(page_num)
        
        return reports
    
    defæå–è¡¨æ ¼ä¸ºDataFrame(self, page_numbers):
        """å°†è¡¨æ ¼æå–ä¸ºpandas DataFrame"""
        all_tables = []
        
        for page_num in page_numbers:
            page = self.doc[page_num - 1]
            tables = page.find_tables()
            
            if tables:
                for table in tables:
                    df = pd.DataFrame(table.extract())
                    df.attrs['page'] = page_num
                    all_tables.append(df)
        
        return all_tables
    
    defæå–å¤§é™†å­å…¬å¸è¡¨æ ¼(self):
        """ä¸“é—¨æå–å¤§é™†å­å…¬å¸ä¿¡æ¯è¡¨æ ¼"""
        # æœç´¢å¤§é™†æŠ•èµ„ä¿¡æ¯
        keywords = [
            "å¤§é™¸æŠ•è³‡è³‡è¨Š",
            "å¤§é™¸è¢«æŠ•è³‡å…¬å¸",
            "ç›´æ¥èµ´å¤§é™¸åœ°å€",
            "é€éç¬¬ä¸‰åœ°å€"
        ]
        
        target_pages = []
        for page_num in range(1, len(self.doc) + 1):
            page = self.doc[page_num - 1]
            text = page.get_text()
            
            for keyword in keywords:
                if keyword in text:
                    target_pages.append(page_num)
                    break
        
        # æå–è¿™äº›é¡µé¢çš„è¡¨æ ¼
        tables = self.æå–è¡¨æ ¼ä¸ºDataFrame(target_pages)
        
        return tables
    
    defå…³é—­(self):
        self.doc.close()

# ä½¿ç”¨ç¤ºä¾‹
extractor = è´¢æŠ¥è¡¨æ ¼æå–å™¨("/tmp/unimicron_2024_tw.pdf")

# æå–å¤§é™†å­å…¬å¸ä¿¡æ¯
df_list = extractor.æå–å¤§é™†å­å…¬å¸è¡¨æ ¼()
print(f"æ‰¾åˆ° {len(df_list)} ä¸ªå¤§é™†å­å…¬å¸ç›¸å…³è¡¨æ ¼")

# ä¿å­˜ä¸ºExcel
for i, df in enumerate(df_list):
    df.to_excel(f"/tmp/å¤§é™†å­å…¬å¸è¡¨æ ¼_{i+1}.xlsx", index=False)
    print(f"âœ… ä¿å­˜: å¤§é™†å­å…¬å¸è¡¨æ ¼_{i+1}.xlsx")
```

---

## ä¸‰ã€PDFå‹ç¼©ä¼˜åŒ–æ–¹æ¡ˆ

### 3.1 å›¾ç‰‡å‹ç¼©ï¼ˆé’ˆå¯¹æ‰«æç‰ˆPDFï¼‰

```python
# pdf_image_compressor.py
import fitz
import io
from PIL import Image

classå›¾ç‰‡å‹ç¼©å™¨:
    """å‹ç¼©PDFä¸­çš„å›¾ç‰‡ä»¥å‡å°æ–‡ä»¶å¤§å°"""
    
    def __init__(self, quality=75, max_width=1920):
        self.quality = quality
        self.max_width = max_width
        
    defå‹ç¼©å›¾ç‰‡(self, image_bytes):
        """å‹ç¼©å•ä¸ªå›¾ç‰‡"""
        img = Image.open(io.BytesIO(image_bytes))
        
        # è°ƒæ•´å¤§å°
        if img.width > self.max_width:
            ratio = self.max_width / img.width
            new_size = (self.max_width, int(img.height * ratio))
            img = img.resize(new_size, Image.LANCZOS)
        
        # å‹ç¼©ä¸ºJPEG
        output = io.BytesIO()
        img.convert('RGB').save(output, format='JPEG', 
                               quality=self.quality, optimize=True)
        
        return output.getvalue()
    
    defå‹ç¼©PDF(self, input_path, output_path):
        """å‹ç¼©PDFä¸­çš„å›¾ç‰‡"""
        doc = fitz.open(input_path)
        
        for page_num, page in enumerate(doc):
            image_list = page.get_images()
            
            for img_index, img in enumerate(image_list):
                xref = img[0]
                base_image = doc.extract_image(xref)
                image_bytes = base_image["image"]
                
                # å‹ç¼©
                compressed_bytes = self.å‹ç¼©å›¾ç‰‡(image_bytes)
                
                # æ›¿æ¢å›¾ç‰‡
                pix = fitz.Pixmap(doc, len(doc.add_image(compressed_bytes)))
                page.insert_image(page.rect, pixmap=pix)
        
        doc.save(output_path, deflate=True)
        doc.close()
        
        return output_path

# ä½¿ç”¨ç¤ºä¾‹
compressor = å›¾ç‰‡å‹ç¼©å™¨(quality=60, max_width=1920)
compressor.å‹ç¼©PDF(
    "/tmp/unimicron_2024_tw.pdf",
    "/tmp/unimicron_compressed.pdf"
)
```

### 3.2 Ghostscriptå‘½ä»¤è¡Œå‹ç¼©

```bash
#!/bin/bash
# compress_pdf.sh

# å®‰è£…Ghostscript
# brew install ghostscript

# å‹ç¼©PDFï¼ˆä¿æŒè´¨é‡ï¼‰
gs -sDEVICE=pdfwrite \
   -dCompatibilityLevel=1.4 \
   -dPDFSETTINGS=/prepress \
   -dNOPAUSE \
   -dQUIET \
   -dBATCH \
   -sOutputFile=compressed.pdf \
   input.pdf

# å‹ç¼©çº§åˆ«è¯´æ˜ï¼š
# /prepress - é«˜è´¨é‡ï¼ˆæ–‡ä»¶è¾ƒå¤§ï¼‰
# /ebook    - ä¸­ç­‰è´¨é‡ï¼ˆæ¨èï¼‰
# /screen   - ä½è´¨é‡ï¼ˆæ–‡ä»¶æœ€å°ï¼‰
# /default  - é»˜è®¤
```

---

## å››ã€RAGæ–‡æ¡£åˆ†ææ–¹æ¡ˆ

### 4.1 PDFè½¬Markdownï¼ˆRAGå‹å¥½æ ¼å¼ï¼‰

```python
# pdf_to_markdown.py
import fitz
import re

classPDFè½¬Markdown:
    """å°†PDFè½¬æ¢ä¸ºMarkdownæ ¼å¼ï¼Œä¾¿äºRAGå¤„ç†"""
    
    def __init__(self, pdf_path):
        self.doc = fitz.open(pdf_path)
        
    defæå–ä¸ºMarkdown(self, output_path):
        """å°†PDFæå–ä¸ºMarkdownæ ¼å¼"""
        markdown_content = []
        
        # æ·»åŠ å…ƒæ•°æ®
        markdown_content.append(f"# {self.doc.metadata.get('title', 'PDFæ–‡æ¡£')}\n")
        markdown_content.append(f"> é¡µæ•°: {len(self.doc)}\n")
        markdown_content.append("---\n")
        
        # æå–æ¯ä¸€é¡µ
        for page_num, page in enumerate(self.doc, 1):
            markdown_content.append(f"\n## ç¬¬ {page_num} é¡µ\n")
            
            # æå–è¡¨æ ¼ï¼ˆä¿ç•™æ ¼å¼ï¼‰
            tables = page.find_tables()
            for table in tables:
                table_data = table.extract()
                markdown_content.append(self._è¡¨æ ¼è½¬Markdown(table_data))
            
            # æå–æ–‡æœ¬
            text = page.get_text()
            text = self._æ¸…ç†æ–‡æœ¬(text)
            markdown_content.append(text)
        
        # å†™å…¥æ–‡ä»¶
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("\n".join(markdown_content))
        
        self.doc.close()
        return output_path
    
    def _è¡¨æ ¼è½¬Markdown(self, table_data):
        """å°†è¡¨æ ¼æ•°æ®è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        if not table_data:
            return ""
        
        lines = []
        max_cols = max(len(row) for row in table_data)
        
        for i, row in enumerate(table_data):
            # è¡¥é½ç©ºç¼º
            row = row + [""] * (max_cols - len(row))
            
            # æ·»åŠ åˆ†éš”çº¿
            if i == 0:
                sep = " | ".join(["---"] * max_cols)
                lines.append(" | ".join(row))
                lines.append(sep)
            else:
                lines.append(" | ".join(row))
        
        return "\n".join(lines) + "\n"
    
    def _æ¸…ç†æ–‡æœ¬(self, text):
        """æ¸…ç†æ–‡æœ¬"""
        # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        # è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
        text = re.sub(r'[ \t]+', ' ', text)
        # ç§»é™¤é¡µç 
        text = re.sub(r'- \d+ -', '', text)
        
        return text.strip()

# ä½¿ç”¨ç¤ºä¾‹
converter = PDFè½¬Markdown("/tmp/unimicron_2024_tw.pdf")
converter.æå–ä¸ºMarkdown("/tmp/æ¬£å…´ç”µå­2024å¹´å¹´æŠ¥.md")
```

### 4.2 è¯­ä¹‰åˆ†å—ï¼ˆSemantic Chunkingï¼‰

```python
# semantic_chunker.py
from typing import List, Dict
import re

classè¯­ä¹‰åˆ†å—å™¨:
    """å°†æ–‡æ¡£æŒ‰è¯­ä¹‰ä¸»é¢˜åˆ†å—ï¼Œä¾¿äºRAGæ£€ç´¢"""
    
    def __init__(self, chunk_size=1000, overlap=100):
        self.chunk_size = chunk_size
        self.overlap = overlap
        
    defåˆ†å—(self, text: str) -> List[Dict]:
        """æŒ‰æ®µè½å’Œæ ‡é¢˜åˆ†å—"""
        chunks = []
        
        # è¯†åˆ«æ ‡é¢˜ï¼ˆè¡Œè¾ƒçŸ­ä¸”åŒ…å«ç‰¹å®šæ¨¡å¼ï¼‰
        lines = text.split('\n')
        current_chunk = ""
        current_section = ""
        
        for i, line in enumerate(lines):
            # æ£€æµ‹æ ‡é¢˜
            if self._æ˜¯æ ‡é¢˜(line):
                # ä¿å­˜ä¹‹å‰çš„å—
                if current_chunk:
                    chunks.append({
                        "content": current_chunk.strip(),
                        "section": current_section
                    })
                current_section = line.strip()
                current_chunk = line + "\n"
            else:
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥åˆ†å—
                if len(current_chunk) > self.chunk_size:
                    # åœ¨æ®µè½è¾¹ç•Œåˆ†å—
                    split_point = current_chunk.rfind('\n\n')
                    if split_point > self.chunk_size * 0.5:
                        chunks.append({
                            "content": current_chunk[:split_point].strip(),
                            "section": current_section
                        })
                        current_chunk = current_chunk[split_point:] + "\n"
                
                current_chunk += line + "\n"
        
        # ä¿å­˜æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append({
                "content": current_chunk.strip(),
                "section": current_section
            })
        
        return chunks
    
    def _æ˜¯æ ‡é¢˜(self, line):
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ ‡é¢˜"""
        # æ ‡é¢˜ç‰¹å¾ï¼šè¾ƒçŸ­ã€ä»¥ç‰¹å®šå­—ç¬¦ç»“å°¾ã€åŒ…å«æ•°å­—ç« èŠ‚
        patterns = [
            r'^#{1,6}\s',           # Markdownæ ‡é¢˜
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\.',  # ä¸­æ–‡æ•°å­—æ ‡é¢˜
            r'^[0-9]+\.[0-9]+',     # æ•°å­—ç« èŠ‚
            r'^[A-Z][A-Z\s]+:$',     # å…¨å¤§å†™æ ‡é¢˜
            r'^è¡¨\s*\d+',            # è¡¨æ ¼æ ‡é¢˜
            r'^åœ–\s*\d+',            # å›¾ç‰‡æ ‡é¢˜
        ]
        
        line = line.strip()
        return any(re.match(p, line) for p in patterns)

# ä½¿ç”¨ç¤ºä¾‹
chunker = è¯­ä¹‰åˆ†å—å™¨(chunk_size=800, overlap=100)

with open("/tmp/æ¬£å…´ç”µå­2024å¹´å¹´æŠ¥.md", "r", encoding="utf-8") as f:
    text = f.read()

chunks = chunker.åˆ†å—(text)
print(f"åˆ†å—å®Œæˆ: {len(chunks)} ä¸ªå—")

# ä¿å­˜åˆ†å—ç»“æœ
import json
with open("/tmp/æ–‡æ¡£åˆ†å—.json", "w", encoding="utf-8") as f:
    json.dump(chunks, f, ensure_ascii=False, indent=2)
```

---

## äº”ã€å·¥ä½œæµæ•´åˆ

### 5.1 å®Œæ•´PDFå¤„ç†å·¥ä½œæµ

```python
# workflow.py
import os
from pathlib import Path

classPDFå¤„ç†å·¥ä½œæµ:
    """æ•´åˆæ‰€æœ‰PDFå¤„ç†åŠŸèƒ½"""
    
    def __init__(self, pdf_path, work_dir="/tmp"):
        self.pdf_path = pdf_path
        self.work_dir = Path(work_dir)
        self.work_dir.mkdir(parents=True, exist_ok=True)
        
    defæ‰§è¡Œå®Œæ•´å¤„ç†(self):
        """æ‰§è¡Œå®Œæ•´çš„PDFå¤„ç†å·¥ä½œæµ"""
        results = {}
        
        # Step 1: å‹ç¼©PDFï¼ˆå¦‚æœå¾ˆå¤§ï¼‰
        if os.path.getsize(self.pdf_path) > 5 * 1024 * 1024:
            print("ğŸ“¦ Step 1: å‹ç¼©PDF...")
            compressed_path = self.work_dir / "compressed.pdf"
            # è°ƒç”¨å‹ç¼©åŠŸèƒ½...
            pdf_path = compressed_path
        else:
            pdf_path = self.pdf_path
        
        # Step 2: æå–æ–‡æœ¬
        print("ğŸ“– Step 2: æå–æ–‡æœ¬...")
        text_output = self.work_dir / "full_text.txt"
        # æå–æ–‡æœ¬...
        
        # Step 3: æå–è¡¨æ ¼
        print("ğŸ“Š Step 3: æå–è¡¨æ ¼...")
        tables_output = self.work_dir / "tables.json"
        # æå–è¡¨æ ¼...
        
        # Step 4: è½¬æ¢ä¸ºMarkdown
        print("ğŸ“ Step 4: è½¬æ¢ä¸ºMarkdown...")
        markdown_output = self.work_dir / "document.md"
        # è½¬æ¢...
        
        # Step 5: è¯­ä¹‰åˆ†å—
        print("ğŸ”— Step 5: è¯­ä¹‰åˆ†å—...")
        chunks_output = self.work_dir / "chunks.json"
        # åˆ†å—...
        
        results = {
            "text": str(text_output),
            "tables": str(tables_output),
            "markdown": str(markdown_output),
            "chunks": str(chunks_output)
        }
        
        return results

# ä¸»ç¨‹åº
if __name__ == "__main__":
    workflow = PDFå¤„ç†å·¥ä½œæµ("/tmp/unimicron_2024_tw.pdf")
    results = workflow.æ‰§è¡Œå®Œæ•´å¤„ç†()
    print("âœ… å¤„ç†å®Œæˆ!")
    for key, path in results.items():
        print(f"  {key}: {path}")
```

---

## å…­ã€å®‰è£…å’Œé…ç½®

### 6.1 å¿…è£…å·¥å…·

```bash
# macOS
brew install poppler  # æä¾›pdfimagesç­‰å·¥å…·
brew install ghostscript  # æä¾›PDFå‹ç¼©

# Pythonåº“
pip install pymupdf    # é«˜æ€§èƒ½PDFå¤„ç†
pip install pandas     # è¡¨æ ¼å¤„ç†
pip install pillow     # å›¾ç‰‡å¤„ç†
pip install openpyxl   # Excelè¾“å‡º
```

### 6.2 å¯é€‰å·¥å…·

```bash
pip install pdfplumber    # å¢å¼ºè¡¨æ ¼æå–
pip install tabula-py     # Javaä¾èµ–çš„è¡¨æ ¼æå–
pip install pdfminer.six  # è¯¦ç»†æ–‡æœ¬åˆ†æ
pip install unstructured  # æ™ºèƒ½æ–‡æ¡£è§£æ
pip install langchain     # RAGæ¡†æ¶
pip install chromadb      # å‘é‡æ•°æ®åº“
```

---

## ä¸ƒã€æ€§èƒ½å¯¹æ¯”

### 7.1 122é¡µPDFå¤„ç†é€Ÿåº¦å¯¹æ¯”

| æ“ä½œ | pdftotext | PyMuPDF | pdfminer |
|------|-----------|---------|----------|
| æå–å…¨éƒ¨æ–‡æœ¬ | ~2ç§’ | ~0.5ç§’ | ~5ç§’ |
| æˆªå›¾å…¨éƒ¨é¡µé¢ | ~30ç§’ | ~8ç§’ | ä¸æ”¯æŒ |
| æå–å…¨éƒ¨è¡¨æ ¼ | ~10ç§’ | ~2ç§’ | ~15ç§’ |
| å†…å­˜å ç”¨ | 150MB | 50MB | 300MB |

### 7.2 ä¼˜åŒ–å»ºè®®

1. **å°æ–‡ä»¶ï¼ˆ<5MBï¼‰**: ç›´æ¥ä½¿ç”¨PyMuPDF
2. **ä¸­ç­‰æ–‡ä»¶ï¼ˆ5-50MBï¼‰**: å…ˆå‹ç¼©å†å¤„ç†
3. **å¤§æ–‡ä»¶ï¼ˆ>50MBï¼‰**: åˆ†é¡µå¤„ç†ï¼Œé¿å…ä¸€æ¬¡æ€§åŠ è½½
4. **è¡¨æ ¼å¯†é›†å‹**: ä½¿ç”¨pdfplumberå¢å¼ºè¡¨æ ¼æ£€æµ‹

---

## å…«ã€æ€»ç»“

### 8.1 æ¨èå·¥å…·æ ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   PDFå¤„ç†å·¥å…·æ ˆ                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ä¸‹è½½å±‚    â”‚  åˆ†æ®µä¸‹è½½å™¨ï¼ˆcurl/requests Rangeå¤´ï¼‰        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  å¤„ç†å±‚    â”‚  PyMuPDF (fitz) - é«˜æ€§èƒ½é¦–é€‰                 â”‚
â”‚            â”‚  pdfplumber - å¢å¼ºè¡¨æ ¼æå–                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  è¾“å‡ºå±‚    â”‚  pandas - è¡¨æ ¼è½¬DataFrame                    â”‚
â”‚            â”‚  Markdown - RAGå‹å¥½æ ¼å¼                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ä¼˜åŒ–å±‚    â”‚  Ghostscript - PDFå‹ç¼©                      â”‚
â”‚            â”‚  Pillow - å›¾ç‰‡å‹ç¼©                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  åˆ†æå±‚    â”‚  LangChain + ChromaDB - RAGæ£€ç´¢              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 å¿«é€Ÿå¼€å§‹

```bash
# 1. å®‰è£…ä¾èµ–
pip install pymupdf pandas openpyxl pillow

# 2. ä½¿ç”¨ç¤ºä¾‹
python3 << 'EOF'
import fitz

# æ‰“å¼€PDF
doc = fitz.open("/tmp/your_large_pdf.pdf")

# å¿«é€Ÿæå–å¤§é™†å­å…¬å¸ä¿¡æ¯
results = {}
for page_num in range(1, len(doc) + 1):
    page = doc[page_num - 1]
    text = page.get_text()
    if "å¤§é™¸å­å…¬å¸" in text:
        results[page_num] = text[:500]  # ä¿å­˜å‰500å­—

print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸å…³é¡µé¢")
doc.close()
EOF
```

### 8.3 ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. âœ… åˆ›å»ºPythonè„šæœ¬æ¨¡æ¿
2. â¬œ å®‰è£…PyMuPDFå’Œä¾èµ–
3. â¬œ æµ‹è¯•å¤§æ–‡ä»¶ï¼ˆ>50MBï¼‰
4. â¬œ é›†æˆåˆ°OpenClaw Skills

---

**æ–¹æ¡ˆè®¾è®¡**: å¯ä¹ (OpenClaw)
**æœ€åæ›´æ–°**: 2026-02-08
